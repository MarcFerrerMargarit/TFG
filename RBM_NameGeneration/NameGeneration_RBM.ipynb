{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is create a RBM machine that generate new names of people, places and things. To achieve this we need to build a model that spits out funny names. \n",
    "\n",
    "The problem that RBMs are trying to solve is learning a probability distribution. Then we want to learn a function P that assings every string a probability according to its plausibility as a particular kind of name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA INPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every data we want to feed into a neural network needs to be transformed into a vector of numbers first. So we represent names as sequences of one-hot vectors of length N, where N is the size of our alphabet.\n",
    "\n",
    "Also we need to fix some maximum string length M ahead of time. Names shortes than M will need to be padded with some special character. \n",
    "\n",
    "To convert the data to our data input we use a codec. This codec allows us to convert our input data to the format we want.\n",
    "\n",
    "Here's the code for the encoder which is used in the original code.\n",
    "\n",
    "The class ShortTextCodes is the encoder which encodes every word to the alphabet space and the maximum lenght of  the world that we set previously. For example, the word \"deb\" will be encoded as \"[4 5 2 26 26]\" where 26 is the index of the character ' ' and the other are the indexes of the characters in the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NonEncodableTextException(Exception):\n",
    "    \n",
    "    def __init__(self, reason=None, *args):\n",
    "        self.reason = reason\n",
    "        super(NonEncodableTextException, self).__init__(*args)\n",
    "\n",
    "class ShortTextCodec(object):\n",
    "    # TODO: problematic if this char appears in the training text\n",
    "    FILLER = '$' \n",
    "\n",
    "    # If a one-hot vector can't be decoded meaningfully, render this char in its place\n",
    "    MYSTERY = '?'\n",
    "\n",
    "    # Backward-compatibility. Was probably a mistake to have FILLER be a class var rather than instance\n",
    "    @property\n",
    "    def filler(self):\n",
    "        if self.__class__.FILLER in self.alphabet:\n",
    "            return self.__class__.FILLER\n",
    "        # Old versions of this class used ' ' as filler\n",
    "        return ' '\n",
    "\n",
    "    def __init__(self, extra_chars, maxlength, minlength=0, preserve_case=False, leftpad=False):\n",
    "        assert 0 <= minlength <= maxlength\n",
    "        if self.FILLER not in extra_chars and maxlength != minlength:\n",
    "            extra_chars = self.FILLER + extra_chars\n",
    "        self.maxlen = maxlength\n",
    "        self.minlen = minlength\n",
    "        self.char_lookup = {}\n",
    "        self.leftpad_ = leftpad\n",
    "        self.alphabet = ''\n",
    "        for i, o in enumerate(range(ord('a'), ord('z') + 1)):\n",
    "            self.char_lookup[chr(o)] = i\n",
    "            self.alphabet += chr(o)\n",
    "        nextidx = len(self.alphabet)\n",
    "        for i, o in enumerate(range(ord('A'), ord('Z') + 1)):\n",
    "            if preserve_case:\n",
    "                self.char_lookup[chr(o)] = nextidx\n",
    "                nextidx += 1\n",
    "                self.alphabet += chr(o)\n",
    "            else:\n",
    "                self.char_lookup[chr(o)] = i\n",
    "\n",
    "        offset = len(self.alphabet)\n",
    "        for i, extra in enumerate(extra_chars):\n",
    "            self.char_lookup[extra] = i + offset\n",
    "            self.alphabet += extra\n",
    "\n",
    "    def debug_description(self):\n",
    "        return ' '.join('{}={}'.format(attr, repr(getattr(self, attr, None))) for attr in ['maxlen', 'minlen', 'leftpad', 'alphabet', 'nchars'])\n",
    "\n",
    "    @property\n",
    "    def leftpad(self):\n",
    "        return getattr(self, 'leftpad_', False)\n",
    "\n",
    "    @property\n",
    "    def nchars(self):\n",
    "        return len(self.alphabet)\n",
    "\n",
    "    @property\n",
    "    def non_special_char_alphabet(self):\n",
    "        return ''.join(c for c in self.alphabet if (c != ' ' and c != self.FILLER)) \n",
    "\n",
    "    def _encode(self, s, padlen):\n",
    "        if len(s) > padlen:\n",
    "            raise NonEncodableTextException(reason='toolong')\n",
    "        padding = [self.char_lookup[self.filler] for _ in range(padlen - len(s))]\n",
    "        try:\n",
    "            payload = [self.char_lookup[c] for c in s]\n",
    "        except KeyError:\n",
    "            raise NonEncodableTextException(reason='illegal_char')\n",
    "        if self.leftpad:\n",
    "            return padding + payload\n",
    "        else:\n",
    "            return payload + padding\n",
    "\n",
    "\n",
    "    def encode(self, s, mutagen=None):\n",
    "        if len(s) > self.maxlen: \n",
    "            raise NonEncodableTextException(reason='toolong')\n",
    "        elif (hasattr(self, 'minlen') and len(s) < self.minlen):\n",
    "            raise NonEncodableTextException(reason='tooshort')\n",
    "        if mutagen:\n",
    "            s = mutagen(s)\n",
    "        return self._encode(s, self.maxlen)\n",
    "\n",
    "    def encode_onehot(self, s):\n",
    "        indices = self.encode(s)\n",
    "        return np.eye(self.nchars)[indices].ravel()\n",
    "\n",
    "    def decode(self, vec, pretty=False, strict=True):\n",
    "        # TODO: Whether we should use 'strict' mode depends on whether the model\n",
    "        # we got this vector from does softmax sampling of visibles. Anywhere this\n",
    "        # is called on fantasy samples, we should use the model to set this param.\n",
    "        if issparse(vec):\n",
    "            vec = vec.toarray().reshape(-1)\n",
    "        assert vec.shape == (self.nchars * self.maxlen,)\n",
    "        chars = []\n",
    "        for position_index in range(self.maxlen):\n",
    "            # Hack - insert a tab between name parts in binomial mode\n",
    "            if isinstance(self, BinomialShortTextCodec) and pretty and position_index == self.maxlen/2:\n",
    "                chars.append('\\t')\n",
    "            subarr = vec[position_index * self.nchars:(position_index + 1) * self.nchars]\n",
    "            if np.count_nonzero(subarr) != 1 and strict:\n",
    "                char = self.MYSTERY\n",
    "            else:\n",
    "                char_index = np.argmax(subarr)\n",
    "                char = self.alphabet[char_index]\n",
    "                if pretty and char == self.FILLER:\n",
    "                    # Hack\n",
    "                    char = ' ' if isinstance(self, BinomialShortTextCodec) else ''\n",
    "            chars.append(char)\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def shape(self):\n",
    "        \"\"\"The shape of a set of RBM inputs given this codecs configuration.\"\"\"\n",
    "        return (self.maxlen, len(self.alphabet))\n",
    "\n",
    "    def mutagen_nudge(self, s):\n",
    "        # Mutate a single character chosen uniformly at random.\n",
    "        # If s is shorter than the max length, include an extra virtual character at the end\n",
    "        i = random.randint(0, min(len(s), self.maxlen-1))\n",
    "        def roll(forbidden):\n",
    "            newchar = random.choice(self.alphabet)\n",
    "            while newchar in forbidden:\n",
    "                newchar = random.choice(self.alphabet)\n",
    "            return newchar\n",
    "                \n",
    "        if i == len(s):\n",
    "            return s + roll(self.FILLER + ' ')\n",
    "        if i == len(s)-1:\n",
    "            replacement = roll(' ' + s[-1])\n",
    "            if replacement == self.FILLER:\n",
    "                return s[:-1]\n",
    "            return s[:-1] + roll(' ' + s[-1])\n",
    "        else:\n",
    "            return s[:i] + roll(s[i] + self.FILLER) + s[i+1:]\n",
    "\n",
    "\n",
    "    def mutagen_silhouettes(self, s):\n",
    "        newchars = []\n",
    "        for char in s:\n",
    "            if char == ' ':\n",
    "                newchars.append(char)\n",
    "            else:\n",
    "                newchars.append(random.choice(self.non_special_char_alphabet))\n",
    "        return ''.join(newchars)\n",
    "        \n",
    "    def mutagen_noise(self, s):\n",
    "        return ''.join(random.choice(self.alphabet) for _ in range(self.maxlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a example of the encoder implemented previously. Also we can see the OneHotEncoder for encode the vector as a matrix as we said previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "codec = ShortTextCodec('',6)\n",
    "f = open(\"./names.txt\")\n",
    "skipped = Counter()\n",
    "vecs = []\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    try:\n",
    "        vecs.append(codec.encode(line))\n",
    "        if(len(vecs) == -1):\n",
    "            break\n",
    "    except NonEncodableTextException as e:\n",
    "            # Too long, or illegal characters\n",
    "            skipped[e.reason] += 1\n",
    "vecs = np.asarray(vecs)\n",
    "vecsOneHot = OneHotEncoder(len(codec.alphabet)).fit_transform(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWN IMPLEMENTATION\n",
    "\n",
    "Now we gonna implement a similar encoder to get the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "def OneHotVector(data, alphabet, maxLength=10):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    dataSplit = data.split(' ')\n",
    "    onehot_encoded = list()\n",
    "    vecs = []\n",
    "    for sntc in dataSplit:\n",
    "        integer_encoded = [char_to_int[char] for char in sntc]\n",
    "        lengthWord = len(integer_encoded)\n",
    "        if(lengthWord < maxLength):\n",
    "            for i in range(maxLength-lengthWord):\n",
    "                integer_encoded.append(26)\n",
    "        tmp = []\n",
    "        for value in integer_encoded:\n",
    "            letter = [0 for _ in range(len(alphabet))]\n",
    "            letter[value] = 1\n",
    "            for valueLetter in letter:\n",
    "                tmp.append(valueLetter)\n",
    "        vecs.append(tmp)\n",
    "    vecs = np.asarray(vecs,  dtype=float)\n",
    "    return vecs\n",
    "\n",
    "def IntegerEncoded(data, alphabet, maxLength=10):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    dataSplit = data.split(' ')\n",
    "    onehot_encoded = list()\n",
    "    vecs = []\n",
    "    for sntc in dataSplit:\n",
    "        integer_encoded = [char_to_int[char] for char in sntc]\n",
    "        lengthWord = len(integer_encoded)\n",
    "        if(lengthWord < maxLength):\n",
    "            for i in range(maxLength-lengthWord):\n",
    "                integer_encoded.append(26)\n",
    "        vecs.append(integer_encoded)\n",
    "    vecs = np.asarray(vecs,  dtype=float)\n",
    "    return vecs\n",
    "\n",
    "def ConverToWords(vec,alphabet='abcdefghijklmnopqrstuvwxyz '):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    finalWord = []\n",
    "    for k in range(len(vec)):\n",
    "        tmp = []\n",
    "        for i in range(len(vec[k])):\n",
    "            if vec[k][i] == 1:\n",
    "                index = i%len(alphabet)\n",
    "                if(index != 26):\n",
    "                    tmp.append(int_to_char[index])\n",
    "        finalWord.append(tmp)\n",
    "    return finalWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version without OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[['t', 'e', 's', 't'], ['e', 'n', 'c', 'o', 'd', 'e', 'r']]\n"
     ]
    }
   ],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
    "data = \"test encoder\"\n",
    "\n",
    "### Data \"test encoder\" converted to oneHotVector\n",
    "vec = OneHotVector(data,alphabet,7)\n",
    "print(vec)\n",
    "### From oneHotVector obtains de words\n",
    "finalWord = ConverToWords(vec)\n",
    "print(finalWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version with OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[['f', 'i', 'n', 'a', 'l'], ['t', 'e', 's', 't']]\n"
     ]
    }
   ],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
    "data = \"test encoder\"\n",
    "vec = IntegerEncoded(data, alphabet, 7)\n",
    "oneHotEncoder = OneHotEncoder(len(alphabet), sparse=False).fit(vec)\n",
    "\n",
    "### Data to oneHotVector\n",
    "vecOneHot = oneHotEncoder.transform(vec)\n",
    "print(vecOneHot)\n",
    "### Convert oneHotVector to words\n",
    "words = \"final test\"\n",
    "newVec = IntegerEncoded(words,alphabet,7)\n",
    "newOneHotVector = oneHotEncoder.transform(newVec)\n",
    "finalWord = ConverToWords(newOneHotVector)\n",
    "print(finalWord)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
